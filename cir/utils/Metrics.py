import torch
import numpy as np
from collections import defaultdict

def compute_mAP_vectorized(sim_mat, is_match):
    """
    [GPU åŠ é€Ÿ] è®¡ç®— Mean Average Precision (mAP)
    é€‚ç”¨äºå• GT å’Œå¤š GT (Soft Match) çš„æƒ…å†µã€‚
    
    Args:
        sim_mat: [N_query, N_gallery] ç›¸ä¼¼åº¦çŸ©é˜µ (è¶Šå¤§è¶Šå¥½)
        is_match: [N_query, N_gallery] Boolean/Int (Ground Truth æ©ç )
    """
    # 1. æ’åº: ç›¸ä¼¼åº¦ä»é«˜åˆ°ä½
    # descending=True
    scores, indices = torch.sort(sim_mat, dim=1, descending=True)
    
    # 2. æ ¹æ®æ’åºç»“æœé‡æ’ Ground Truth
    # gather: å°† is_match æŒ‰ç…§ indices çš„é¡ºåºé‡æ–°æ’åˆ—
    # gts[i][j] = 1 è¡¨ç¤ºç¬¬ i ä¸ª Query çš„ç¬¬ j åé¢„æµ‹ç»“æœæ˜¯æ­£ç¡®çš„
    gts = torch.gather(is_match.float(), 1, indices)
    
    # 3. è®¡ç®— Precision
    # cumsum: è®¡ç®—æ¯ä¸€ä½çš„ç´¯è®¡æ­£ç¡®æ•° (å³ TP @ k)
    cumsum = torch.cumsum(gts, dim=1)
    
    # ranks: ç”Ÿæˆæ’å [1, 2, 3, ..., M]
    ranks = torch.arange(1, gts.size(1) + 1, device=gts.device).float().unsqueeze(0)
    
    # precision @ k = (TP @ k) / k
    precision = cumsum / ranks
    
    # 4. è®¡ç®— Average Precision (AP)
    # AP = sum(precision * is_relevant) / num_total_relevant
    # åªæœ‰ç›¸å…³ä½ç½®çš„ precision æ‰å‚ä¸æ±‚å’Œ
    ap_sum = (precision * gts).sum(dim=1)
    num_relevant = gts.sum(dim=1)
    
    # 5. é¿å…é™¤ä»¥ 0 (å¯¹äºæ²¡æœ‰ä»»ä½•æ­£ç¡®ç­”æ¡ˆçš„ Queryï¼ŒAP=0)
    mask = num_relevant > 0
    ap = torch.zeros_like(ap_sum)
    ap[mask] = ap_sum[mask] / num_relevant[mask]
    
    # 6. Mean AP
    return ap.mean().item()

def compute_rank_k_vectorized(sim_mat, is_match, ks=[1, 5, 10]):
    """
    [GPU åŠ é€Ÿ] è®¡ç®— R@K (CMC Rank-k / Hit Rate)
    å®šä¹‰ï¼šåªè¦ Top-K ç»“æœä¸­åŒ…å« *è‡³å°‘ä¸€ä¸ª* æ­£ç¡®ç­”æ¡ˆï¼Œå°±ç®—å‘½ä¸­ (Hit)ã€‚
    """
    # 1. æ’åº
    _, indices = torch.sort(sim_mat, dim=1, descending=True)
    
    # 2. é‡æ’ GT
    gts = torch.gather(is_match.float(), 1, indices)
    
    results = {}
    for k in ks:
        if k > gts.size(1):
            results[f'R{k}'] = 100.0
            continue
            
        # å–å‰ K åˆ—
        top_k = gts[:, :k]
        
        # åªè¦è¡Œå’Œ > 0ï¼Œè¯´æ˜å‘½ä¸­äº†è‡³å°‘ 1 ä¸ª
        hits = (top_k.sum(dim=1) > 0).float()
        
        # å¹³å‡å‘½ä¸­ç‡
        results[f'R{k}'] = hits.mean().item() * 100.0
        
    return results

def _compute_on_device(device, q_feats, g_feats, q_meta, g_meta, q_tasks):
    """
    å†…éƒ¨è®¡ç®—å‡½æ•°ï¼šè´Ÿè´£æ•°æ®æ¬è¿ã€Mask ç”Ÿæˆå’ŒæŒ‡æ ‡è®¡ç®—
    """
    # 1. ç‰¹å¾æ¬è¿ä¸å½’ä¸€åŒ–
    q_feats = q_feats.to(device)
    g_feats = g_feats.to(device)
    
    q_feats = torch.nn.functional.normalize(q_feats, p=2, dim=1)
    g_feats = torch.nn.functional.normalize(g_feats, p=2, dim=1)
    
    # 2. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    sim_mat = torch.mm(q_feats, g_feats.t())
    
    # 3. æå–å…ƒæ•°æ® (CPU Numpy å¤„ç†å­—ç¬¦ä¸²ï¼Œç”Ÿæˆ Bool Mask åè½¬ GPU)
    q_sids = np.array([m['sid'] for m in q_meta])
    g_sids = np.array([m['sid'] for m in g_meta])
    
    q_views = np.array([m['view'] for m in q_meta])
    g_views = np.array([m['view'] for m in g_meta])
    
    # === ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šæ¨¡ç³ŠåŒ¹é… Condition ===
    # å¿½ç•¥åç¼€ (å¦‚ -01, -02)ï¼Œåªä¿ç•™ç±»åˆ«å‰ç¼€ (nm, bg, cl)
    # è¿™æ · "bg-01" å’Œ "bg-02" ä¼šè¢«è§†ä¸ºç›¸åŒçš„ Soft Label
    q_conds = np.array([str(m['cond']).split('-')[0] for m in q_meta])
    g_conds = np.array([str(m['cond']).split('-')[0] for m in g_meta])
    
    # 4. ç”Ÿæˆ GPU å¸ƒå°”æ©ç  (Ground Truth)
    # Broadcasting: [N, 1] == [1, M] -> [N, M]
    match_id = torch.from_numpy(q_sids[:, None] == g_sids[None, :]).to(device)
    match_cond = torch.from_numpy(q_conds[:, None] == g_conds[None, :]).to(device)
    match_view = torch.from_numpy(q_views[:, None] == g_views[None, :]).to(device)
    
    # 5. åˆ†ä»»åŠ¡è®¡ç®—
    task_groups = defaultdict(list)
    for idx, task in enumerate(q_tasks):
        task_groups[task].append(idx)
        
    # å¢åŠ  Overall ç»„
    if "Overall" not in task_groups:
        task_groups["Overall"] = list(range(len(q_feats)))
        
    final_results = {}
    
    for task, indices in task_groups.items():
        if len(indices) == 0: continue
        
        # æå–å½“å‰ä»»åŠ¡çš„å­é›†ç´¢å¼•
        indices_t = torch.tensor(indices, device=device)
        
        # åˆ‡ç‰‡çŸ©é˜µ
        sub_sim = sim_mat[indices_t]
        sub_match_id = match_id[indices_t]
        sub_match_cond = match_cond[indices_t]
        sub_match_view = match_view[indices_t]
        
        # === å®šä¹‰åŒ¹é…æ ‡å‡† ===
        
        # A. Strict: ä¸¥æ ¼åŒ¹é… (äººå¯¹ + çŠ¶æ€å¯¹ + è§†è§’å¯¹)
        # è¿™é‡Œçš„çŠ¶æ€ä¹Ÿæ˜¯æ¨¡ç³ŠåŒ¹é…åçš„ (bg)ï¼Œè¿™æ²¡é—®é¢˜ï¼Œå› ä¸º Strict ä¸»è¦é  View æ¥çº¦æŸå”¯ä¸€æ€§
        gt_strict = sub_match_id & sub_match_cond & sub_match_view
        
        # B. Soft: å®½æ¾åŒ¹é… (äººå¯¹ + çŠ¶æ€å¯¹) -> å¿½ç•¥è§†è§’ï¼Œå¿½ç•¥åºåˆ—å·å·®å¼‚
        # è¿™ä¸‹ bg-01 æœå‡º bg-02 ä¹Ÿèƒ½ç®—å¯¹äº†ï¼
        gt_soft = sub_match_id & sub_match_cond
        
        # C. ID: èº«ä»½åŒ¹é… (äººå¯¹)
        gt_id = sub_match_id
        
        # === è®¡ç®—æŒ‡æ ‡ ===
        metrics = {'Count': len(indices)}
        
        # 1. Strict
        res_strict = compute_rank_k_vectorized(sub_sim, gt_strict)
        res_strict['mAP'] = compute_mAP_vectorized(sub_sim, gt_strict) * 100
        metrics['Strict'] = res_strict
        
        # 2. Soft
        res_soft = compute_rank_k_vectorized(sub_sim, gt_soft)
        res_soft['mAP'] = compute_mAP_vectorized(sub_sim, gt_soft) * 100
        metrics['Soft'] = res_soft
        
        # 3. ID
        res_id = compute_rank_k_vectorized(sub_sim, gt_id)
        res_id['mAP'] = compute_mAP_vectorized(sub_sim, gt_id) * 100
        metrics['ID'] = res_id
        
        final_results[task] = metrics
        
    return final_results

def compute_hierarchical_metrics(q_feats, g_feats, q_meta, g_meta, q_tasks):
    """
    å¯¹å¤–æ¥å£ï¼šè‡ªåŠ¨æ˜¾å­˜ä¿æŠ¤
    """
    try:
        if torch.cuda.is_available():
            return _compute_on_device(torch.device("cuda"), q_feats, g_feats, q_meta, g_meta, q_tasks)
        else:
            return _compute_on_device(torch.device("cpu"), q_feats, g_feats, q_meta, g_meta, q_tasks)
    except RuntimeError as e:
        if "out of memory" in str(e):
            print("\nâš ï¸ [Metrics] GPU OOM! Swapping to CPU for evaluation...")
            torch.cuda.empty_cache()
            return _compute_on_device(torch.device("cpu"), q_feats, g_feats, q_meta, g_meta, q_tasks)
        else:
            raise e