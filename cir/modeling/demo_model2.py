import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import CLIPModel

class Combiner(nn.Module):
    """
    Combiner module which once trained fuses textual and visual information
    """

    def __init__(self, clip_feature_dim: int, projection_dim: int, hidden_dim: int):
        """
        :param clip_feature_dim: CLIP input feature dimension
        :param projection_dim: projection dimension
        :param hidden_dim: hidden dimension
        """
        super(Combiner, self).__init__()
        # 1. ç‰¹å¾æŠ•å½±å±‚ (å°† CLIP ç‰¹å¾æŠ•å½±åˆ° projection_dim)
        self.text_projection_layer = nn.Linear(clip_feature_dim, projection_dim)
        self.image_projection_layer = nn.Linear(clip_feature_dim, projection_dim)

        self.dropout1 = nn.Dropout(0.5)
        self.dropout2 = nn.Dropout(0.5)
        
        # 2. èåˆå±‚ (è¾“å…¥ä¸º 2 * projection_dim)
        self.combiner_layer = nn.Linear(projection_dim * 2, hidden_dim)
        self.output_layer = nn.Linear(hidden_dim, clip_feature_dim) # è¾“å‡ºå›åŸå°ºå¯¸

        self.dropout3 = nn.Dropout(0.5)
        
        # 3. åŠ¨æ€æ ‡é‡ï¼ˆDynamic Scalarï¼‰ç”¨äºæ®‹å·®è¿æ¥çš„æƒé‡
        self.dynamic_scalar = nn.Sequential(
            nn.Linear(projection_dim * 2, hidden_dim), 
            nn.ReLU(), 
            nn.Dropout(0.5),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )

        # æ¸©åº¦ç³»æ•°ï¼Œè¿™é‡Œä¿ç•™åŸå€¼ï¼Œä½†é€šå¸¸åœ¨ GaitCIRModel ä¸­å¤„ç†
        self.logit_scale = 100 

    # âš ï¸ å…³é”®ä¿®æ”¹ 1: å°† combine_features æ”¹ä¸º forward
    def forward(self, image_features, text_features):
        """
        Cobmine the reference image features and the caption features. It outputs the predicted features
        :param image_features: CLIP reference image features (agg_feat)
        :param text_features: CLIP relative caption features (txt_feat)
        :return: predicted features (query_feat)
        """
        # æŠ•å½±ä¸æ¿€æ´»
        text_projected_features = self.dropout1(F.relu(self.text_projection_layer(text_features)))
        image_projected_features = self.dropout2(F.relu(self.image_projection_layer(image_features)))

        # æ‹¼æ¥æŠ•å½±åçš„ç‰¹å¾
        raw_combined_features = torch.cat((text_projected_features, image_projected_features), -1)
        
        # èåˆ MLP
        combined_features = self.dropout3(F.relu(self.combiner_layer(raw_combined_features)))
        
        # è®¡ç®—åŠ¨æ€æ ‡é‡ sigma
        dynamic_scalar = self.dynamic_scalar(raw_combined_features)
        
        # æœ€ç»ˆèåˆï¼šè¾“å‡º MLP + åŠ¨æ€åŠ æƒæ®‹å·®è¿æ¥
        # Query = MLP_output + sigma * Txt_feat + (1 - sigma) * Img_feat
        output = self.output_layer(combined_features) + dynamic_scalar * text_features + (
                        1 - dynamic_scalar) * image_features
                        
        # å½’ä¸€åŒ– (Combiner å†…éƒ¨å®Œæˆ)
        return F.normalize(output)

class GaitCIRModel(nn.Module):
    def __init__(self, model_id="openai/clip-vit-base-patch32"):
        super().__init__()
        print(f"Loading CLIP: {model_id}...")
        self.clip = CLIPModel.from_pretrained(model_id)
        
        # â„ï¸ å†»ç»“ CLIP è§†è§‰å’Œæ–‡æœ¬éƒ¨åˆ†ï¼Œåªè®­ç»ƒ Combiner
        for param in self.clip.parameters():
            param.requires_grad = False
            
        # åˆå§‹åŒ–ç»„ä»¶
        self.feature_dim = self.clip.projection_dim # 512
        
        # âš ï¸ å…³é”®ä¿®æ”¹ 2: ä¼ å…¥æ–°çš„ Combiner æ‰€éœ€çš„ä¸‰ä¸ªå‚æ•°
        self.combiner = Combiner(
            clip_feature_dim=self.feature_dim, 
            projection_dim=self.feature_dim, 
            hidden_dim=2048 # æ²¿ç”¨åŸæ¥çš„ 2048
        )
        
        # å¯å­¦ä¹ çš„æ¸©åº¦ç³»æ•°
        self.logit_scale = nn.Parameter(torch.ones([]) * 2.6592)

    def extract_img_feature(self, pixel_values):
        """ å•å¸§ç‰¹å¾æå–: [N, 3, H, W] -> [N, 512] """
        with torch.no_grad():
            feat = self.clip.get_image_features(pixel_values)
        return F.normalize(feat, dim=-1)

    def extract_txt_feature(self, input_ids, attention_mask):
        """ æ–‡æœ¬ç‰¹å¾æå–: [B, L] -> [B, 512] """
        with torch.no_grad():
            feat = self.clip.get_text_features(input_ids, attention_mask)
        return F.normalize(feat, dim=-1)

    def aggregate_features(self, inputs, batch_size, frames_num):
        """
        ğŸ”¥ æ ¸å¿ƒå‡çº§ï¼šGaitSet é£æ ¼çš„æ—¶åºèšåˆ (Set Pooling)
        æ”¯æŒè¾“å…¥ 'Image Tensor' æˆ– 'Feature Tensor'ï¼Œè‡ªåŠ¨å¤„ç†ã€‚
        """
        # 1. å¦‚æœè¾“å…¥æ˜¯å›¾ç‰‡ [B*T, 3, H, W]ï¼Œå…ˆæå–ç‰¹å¾
        if inputs.dim() == 4:
            features = self.extract_img_feature(inputs) # -> [B*T, 512]
        else:
            features = inputs # å·²ç»æ˜¯ [B*T, 512] æˆ– [B, T, 512]

        # 2. ç»Ÿä¸€ç»´åº¦ -> [B, T, D]
        if features.dim() == 2:
            features = features.view(batch_size, frames_num, -1)
            
        # 3. Max Pooling (GaitSet ä¹Ÿæ˜¯ç”¨çš„ Max)
        agg_feat = features.max(dim=1)[0] # -> [B, 512]
        
        # 4. âš ï¸ å†æ¬¡å½’ä¸€åŒ– (Pooling åæ¨¡é•¿ä¼šå˜ï¼Œå¿…é¡» Re-Norm)
        return F.normalize(agg_feat, dim=-1)

    def forward(self, ref_input, input_ids, attention_mask):
        """
        è®­ç»ƒå‰å‘ä¼ æ’­ï¼šè®¡ç®— Query Embedding
        è‡ªåŠ¨åˆ¤æ–­ ref_input æ˜¯å›¾ç‰‡è¿˜æ˜¯ç‰¹å¾
        """
        batch_size = input_ids.size(0)
        
        # === 1. è§†è§‰å¤„ç† (Extract + Aggregate) ===
        if ref_input.dim() == 4:
            # Image Mode: [N, 3, H, W] -> éœ€è¦è®¡ç®— T
            total_imgs = ref_input.size(0)
            frames_num = total_imgs // batch_size
            ref_agg = self.aggregate_features(ref_input, batch_size, frames_num)
            
        elif ref_input.dim() == 3:
            # Feature Mode: [B, T, D]
            frames_num = ref_input.size(1)
            ref_agg = self.aggregate_features(ref_input, batch_size, frames_num)
            
        else:
            raise ValueError(f"Unknown input shape: {ref_input.shape}")

        # === 2. æ–‡æœ¬å¤„ç† ===
        txt_feat = self.extract_txt_feature(input_ids, attention_mask)
        
        # === 3. èåˆ (Ref + Text -> Query) ===
        # èåˆå™¨å†…éƒ¨å·²ç»åŒ…å«äº†å½’ä¸€åŒ–æ“ä½œ
        query_feat = self.combiner(ref_agg, txt_feat)
        
        # è¾“å‡ºå½’ä¸€åŒ– (å·²ç§»é™¤ï¼Œå› ä¸º Combiner å†…éƒ¨å·²å®Œæˆ)
        return query_feat